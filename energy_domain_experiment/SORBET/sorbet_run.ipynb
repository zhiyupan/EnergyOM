{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/sorbet_data\"   # 你的数据目录\n",
        "SOURCE_OWL = \"saref4bldg.rdf\"                         # 作为 source 的本体文件名（可改）\n",
        "TARGET_OWL = \"Sargon.owl\"                     # 作为 target 的本体文件名（可改）\n",
        "\n",
        "import os, pathlib\n",
        "assert os.path.exists(os.path.join(BASE_DIR, SOURCE_OWL)), f\"缺少 {SOURCE_OWL}\"\n",
        "assert os.path.exists(os.path.join(BASE_DIR, TARGET_OWL)), f\"缺少 {TARGET_OWL}\"\n",
        "pathlib.Path(f\"{BASE_DIR}/refs_equiv\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/sorbet_data/result_alignments\"  # 结果直接写入Drive\n",
        "pathlib.Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"✔ 数据&结果目录OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDOuZZCSDyDq",
        "outputId": "78abae59-0f8e-40b9-f264-97bf6d4fe560"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✔ 数据&结果目录OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf SORBET_ISWC23\n",
        "!git clone https://github.com/Lama-West/SORBET_ISWC23.git\n",
        "%cd /content/SORBET_ISWC23\n",
        "!ls -la | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPPgaQWTEgTq",
        "outputId": "83a1e9ba-6f08-4c51-acff-8ae44abebe34"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'SORBET_ISWC23'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 97 (delta 33), reused 94 (delta 30), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 222.54 KiB | 22.25 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/SORBET_ISWC23\n",
            "total 56\n",
            "drwxr-xr-x 5 root root  4096 Oct 10 21:41 .\n",
            "drwxr-xr-x 1 root root  4096 Oct 10 21:41 ..\n",
            "-rw-r--r-- 1 root root  7019 Oct 10 21:41 environment.yml\n",
            "drwxr-xr-x 2 root root  4096 Oct 10 21:41 figures\n",
            "drwxr-xr-x 8 root root  4096 Oct 10 21:41 .git\n",
            "-rw-r--r-- 1 root root   812 Oct 10 21:41 .gitignore\n",
            "-rw-r--r-- 1 root root  8549 Oct 10 21:41 README.md\n",
            "-rw-r--r-- 1 root root 10158 Oct 10 21:41 requirements.txt\n",
            "drwxr-xr-x 7 root root  4096 Oct 10 21:41 src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 先卸掉常见冲突包\n",
        "!python -m pip uninstall -y -q opencv-python opencv-python-headless opencv-contrib-python tsfresh thinc arviz mizani plotnine umap-learn imbalanced-learn dopamine-rl\n",
        "\n",
        "# 安装核心依赖（与本仓库配合良好的一组版本）\n",
        "!python -m pip install -q --no-cache-dir \\\n",
        "  \"numpy==1.26.4\" \"scipy==1.13.1\" \"pandas==2.2.2\" \"scikit-learn==1.4.2\" \\\n",
        "  \"w3lib==2.2.1\" \"autocorrect==2.6.1\" \"nltk==3.9.1\" \"owlready2==0.46\" \\\n",
        "  \"rdflib==7.0.0\" \"gensim==4.3.2\" \"tqdm==4.66.5\" \"networkx==3.2.1\"\n",
        "\n",
        "# 下载 NLTK 词表\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "print(\"✔ 依赖安装完成\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awQtq3AaEi8Q",
        "outputId": "b7125453-d800-4167-b8d1-935d2f3de8f0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tsfresh as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m312.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m178.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m216.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m189.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gensim (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✔ 依赖安装完成\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "cfg = {\n",
        "  \"General\": { \"model\": \"sentence-transformers/all-MiniLM-L6-v2\" },\n",
        "  \"energy_pair\": {\n",
        "    \"ontologies_folder\": BASE_DIR,\n",
        "    \"ontologies\": [SOURCE_OWL, TARGET_OWL],\n",
        "    \"alignments_folder\": f\"{BASE_DIR}/refs_equiv\",\n",
        "    \"ontologies_in_alignment\": [\n",
        "        os.path.splitext(SOURCE_OWL)[0],\n",
        "        os.path.splitext(TARGET_OWL)[0]\n",
        "    ],\n",
        "    \"alignments\": [],          # 无训练标注\n",
        "    \"test_alignments\": \"\",     # 先留空，下一格生成最小金标后再指向\n",
        "    \"parsing_parameters\": {\n",
        "      \"use_synonyms\": 1,\n",
        "      \"exclude_classes\": [],\n",
        "      \"subclass_of_properties\": [\"rdfs:subClassOf\"],\n",
        "      \"autocorrect\": 0,\n",
        "      \"synonym_extension\": 0\n",
        "    }\n",
        "  }\n",
        "}\n",
        "with open('config.json','w',encoding='utf-8') as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "print(\"✔ config.json 已写好\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-jKcLhwEp0y",
        "outputId": "6d5a1bfb-63d2-445a-d8e0-740d901beb91"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ config.json 已写好\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph, RDF, OWL\n",
        "import os\n",
        "\n",
        "left  = os.path.join(BASE_DIR, SOURCE_OWL)\n",
        "right = os.path.join(BASE_DIR, TARGET_OWL)\n",
        "refs  = os.path.join(BASE_DIR, \"refs_equiv\")\n",
        "os.makedirs(refs, exist_ok=True)\n",
        "\n",
        "def pick_class_iri(path):\n",
        "    g = Graph(); g.parse(path)\n",
        "    for s in g.subjects(RDF.type, OWL.Class):\n",
        "        return str(s)\n",
        "    return str(OWL.Thing)\n",
        "\n",
        "src_iri = pick_class_iri(left)   # source 的任意一个类\n",
        "tgt_iri = pick_class_iri(right)  # target 的任意一个类\n",
        "\n",
        "xml = f'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
        "<rdf:RDF xmlns=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment#\"\n",
        "         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
        "  <Alignment>\n",
        "    <xml>yes</xml><level>0</level><type>**</type>\n",
        "    <onto1><Ontology rdf:about=\"file://{left}\"/></onto1>\n",
        "    <onto2><Ontology rdf:about=\"file://{right}\"/></onto2>\n",
        "    <map>\n",
        "      <Cell>\n",
        "        <entity1 rdf:resource=\"{src_iri}\"/>\n",
        "        <entity2 rdf:resource=\"{tgt_iri}\"/>\n",
        "        <relation>=</relation><measure>1.0</measure>\n",
        "      </Cell>\n",
        "    </map>\n",
        "  </Alignment>\n",
        "</rdf:RDF>\n",
        "'''\n",
        "gold_path = os.path.join(refs, \"minimal_gold.rdf\")\n",
        "with open(gold_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(xml)\n",
        "\n",
        "# 更新 config.json 指向这个测试集\n",
        "import json\n",
        "with open('config.json','r',encoding='utf-8') as f:\n",
        "    cfg = json.load(f)\n",
        "cfg[\"energy_pair\"][\"test_alignments\"] = \"minimal_gold.rdf\"\n",
        "with open('config.json','w',encoding='utf-8') as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✔ 最小金标写好：\", gold_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pti2crRbE8Q_",
        "outputId": "37b8a80d-9fa3-4aa7-cdcd-fcb3289af0af"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ 最小金标写好： /content/drive/MyDrive/sorbet_data/refs_equiv/minimal_gold.rdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, re, json, os\n",
        "%cd /content/SORBET_ISWC23\n",
        "\n",
        "# ===== A) train.py：轨道 energy_pair、只推理、阈值、结果路径、关闭负采样 =====\n",
        "p = pathlib.Path(\"src/train.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 轨道固定 energy_pair\n",
        "txt = re.sub(r'tracks\\s*=\\s*\\[[^\\]]*\\]', 'tracks=[\"energy_pair\"]', txt)\n",
        "txt = re.sub(r'(TrainPipeline\\(\\s*config\\s*,\\s*)\\[[^\\]]*\\]', r'\\1[\"energy_pair\"]', txt)\n",
        "\n",
        "# 只推理：epochs=0\n",
        "if \"epochs=\" in txt:\n",
        "    txt = re.sub(r'epochs\\s*=\\s*\\d+', 'epochs=0', txt)\n",
        "else:\n",
        "    txt = txt.replace(\"TrainPipeline(\", \"TrainPipeline(epochs=0, \", 1)\n",
        "\n",
        "# 推理阈值更宽松一些（多产出候选）\n",
        "if \"inference_config\" in txt:\n",
        "    txt = re.sub(r'\"thresholds\"\\s*:\\s*\\[[^\\]]*\\]', '\"thresholds\":[0.50,0.55,0.60,0.65]', txt)\n",
        "else:\n",
        "    txt = re.sub(r'TrainPipeline\\(', 'TrainPipeline(inference_config={\"matcher\": GreedyMatcher(), \"thresholds\":[0.50,0.55,0.60,0.65]}, ', txt, count=1)\n",
        "\n",
        "# 结果直写到 Drive & 打开 tsv/rdf\n",
        "if \"metrics_config\" in txt:\n",
        "    txt = re.sub(r'\"results_files_path\"\\s*:\\s*\"[^\"]*\"', f'\"results_files_path\":\"{RESULTS_DIR}\"', txt)\n",
        "    txt = re.sub(r'\"write_tsv\"\\s*:\\s*False', '\"write_tsv\":True', txt)\n",
        "    txt = re.sub(r'\"write_rdf\"\\s*:\\s*False', '\"write_rdf\":True', txt)\n",
        "else:\n",
        "    txt = re.sub(r'TrainPipeline\\(',\n",
        "                 f'TrainPipeline(metrics_config={{\"results_files_path\":\"{RESULTS_DIR}\",\"write_tsv\":True,\"write_rdf\":True,\"hits\":[1,5,10]}}, ',\n",
        "                 txt, count=1)\n",
        "\n",
        "# 明确“关闭硬负样本/半负样本”，并让一个批次全是正样本（不生成负样本）\n",
        "def ensure_kv(s, key, val):\n",
        "    if f'\"{key}\"' in s:\n",
        "        return re.sub(rf'\"{key}\"\\s*:\\s*[^,}}]+', f'\"{key}\": {val}', s)\n",
        "    return s\n",
        "txt = ensure_kv(txt, \"negative_sampling_strategy\", '\"none\"')\n",
        "txt = ensure_kv(txt, \"no_hard_negative_samples\", \"True\")  # 关闭硬负样本\n",
        "txt = ensure_kv(txt, \"semi_negative_hop_strategy\", '\"none\"')  # 关闭半负样本\n",
        "txt = ensure_kv(txt, \"batch_size\", \"16\")\n",
        "txt = ensure_kv(txt, \"n_alignments_per_batch\", \"16\")  # 批内全是正样本 → 不会生成负样本\n",
        "\n",
        "# run_tasks 打开（跑推理）\n",
        "txt = re.sub(r'run_tasks\\s*=\\s*False', 'run_tasks=True', txt)\n",
        "p.write_text(txt, encoding=\"utf-8\")\n",
        "print(\"✔ train.py 已修补\")\n",
        "\n",
        "# ===== B) train_pipeline.py：epochs=0 时不写 embedding/不存模型，避免未定义 epoch =====\n",
        "tp = pathlib.Path(\"src/train_pipeline.py\")\n",
        "t = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "t = re.sub(r'self\\s*\\.\\s*save_model\\s*\\(\\s*epoch\\s*\\)', 'self.save_model(locals().get(\"epoch\",-1))', t)\n",
        "t = re.sub(r'(def\\s+save_model\\s*\\(\\s*self\\s*,\\s*epoch[^\\)]*\\)\\s*:\\s*\\n)',\n",
        "           r'\\1        if epoch is None or (isinstance(epoch,int) and epoch<0):\\n            return\\n', t, count=1)\n",
        "t = re.sub(r'self\\._write_embedding_file\\(([^)]*?),\\s*epoch\\)',\n",
        "           r'self._write_embedding_file(\\1, locals().get(\"epoch\",-1))', t)\n",
        "t = re.sub(r'(def\\s+_write_embedding_file\\s*\\(\\s*self\\s*,\\s*onto\\s*,\\s*model\\s*,\\s*epoch[^\\)]*\\)\\s*:\\s*\\n)',\n",
        "           r'\\1        if epoch is None or (isinstance(epoch,int) and epoch<0):\\n            return\\n', t, count=1)\n",
        "tp.write_text(t, encoding=\"utf-8\")\n",
        "print(\"✔ train_pipeline.py 已修补\")\n",
        "\n",
        "# ===== C) metrics.py：无金标时不除以 0 =====\n",
        "mp = pathlib.Path(\"src/metrics.py\")\n",
        "m = mp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "if \"denom = max(1, equal_alignments_len)\" not in m:\n",
        "    m = re.sub(r'(equal_alignments_len\\s*=\\s*[^\\n]+)',\n",
        "               r'\\1\\n        denom = max(1, equal_alignments_len)', m, count=1)\n",
        "m = re.sub(r'/\\s*equal_alignments_len', r'/ denom', m)\n",
        "m = re.sub(r'logger\\.info\\(f\"Hits@\\{hit\\}\\s*:.*?\\)\\)',\n",
        "           'logger.info(f\"Hits@{hit}    : \" + (str(sum([result.hits[i] for result in self.subtrack_metrics]) / denom) '\n",
        "           'if equal_alignments_len > 0 else \"NA (no gold)\"))', m)\n",
        "mp.write_text(m, encoding=\"utf-8\")\n",
        "print(\"✔ metrics.py 已修补\")\n",
        "\n",
        "# ===== D) ontology.py：补齐缺失方法 + get_id_label 始终返回 list =====\n",
        "op = pathlib.Path(\"src/batch_loaders/ontology_parsing/ontology.py\")\n",
        "o = op.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "o = re.sub(r\"def\\s+get_id_label\\s*\\(\\s*self\\s*,\\s*id\\s*\\)\\s*:[\\s\\S]*?^(?=def\\s|\\Z)\",\n",
        "           \"\"\"def get_id_label(self, id):\n",
        "        try:\n",
        "            vals = getattr(self, \"id_to_label_dict\", {}).get(id)\n",
        "        except Exception:\n",
        "            vals = None\n",
        "        if vals is None:\n",
        "            sid = str(id)\n",
        "            local = sid.split(\"#\")[-1].split(\"/\")[-1]\n",
        "            return [local]\n",
        "        return vals if isinstance(vals, list) else [str(vals)]\n",
        "\"\"\", o, flags=re.MULTILINE)\n",
        "\n",
        "def inject_after_class(src, body):\n",
        "    return re.sub(r'(class\\s+Ontology\\s*\\(.*?\\):\\s*\\n)', r'\\1' + body + '\\n', src, count=1)\n",
        "if \"def construct_ID_label_dict\" not in o:\n",
        "    o = inject_after_class(o, '''\n",
        "    def construct_ID_label_dict(self):\n",
        "        try:\n",
        "            if hasattr(self,\"construct_mapping_dict\"):\n",
        "                self.construct_mapping_dict()\n",
        "        except Exception:\n",
        "            pass\n",
        "        if not hasattr(self,\"id_to_label_dict\") or not isinstance(self.id_to_label_dict,dict):\n",
        "            self.id_to_label_dict = {}\n",
        "        return None\n",
        "''')\n",
        "if \"def construct_synonym_controlled_randomness\" not in o:\n",
        "    o = inject_after_class(o, '''\n",
        "    def construct_synonym_controlled_randomness(self):\n",
        "        if not hasattr(self,\"synonym_controlled_randomness\"):\n",
        "            self.synonym_controlled_randomness = 0\n",
        "        return None\n",
        "''')\n",
        "helpers = \"\"\n",
        "if \"def _get_neighbors_generic(\" not in o:\n",
        "    helpers += '''\n",
        "    def _get_neighbors_generic(self, node):\n",
        "        sid = str(node); nbrs=set()\n",
        "        for attr in [\"G\",\"graph\",\"ontology_graph\",\"nx_graph\"]:\n",
        "            g = getattr(self, attr, None)\n",
        "            if g is None: continue\n",
        "            try:\n",
        "                if hasattr(g,\"neighbors\"): nbrs.update(map(str, getattr(g,\"neighbors\")(sid)))\n",
        "                if hasattr(g,\"predecessors\"): nbrs.update(map(str, getattr(g,\"predecessors\")(sid)))\n",
        "                if hasattr(g,\"successors\"): nbrs.update(map(str, getattr(g,\"successors\")(sid)))\n",
        "            except: pass\n",
        "        for dname in [\"children_map\",\"parents_map\",\"adjacency\",\"adj_map\",\"neighbors_dict\",\"neighbours_dict\"]:\n",
        "            d = getattr(self, dname, None)\n",
        "            if isinstance(d,dict) and sid in d:\n",
        "                try:\n",
        "                    v = d[sid]\n",
        "                    nbrs.update(map(str, v if isinstance(v,(list,set,tuple)) else [v]))\n",
        "                except: pass\n",
        "        return nbrs\n",
        "'''\n",
        "if \"def node_has_neighbours(\" not in o:\n",
        "    helpers += '''\n",
        "    def node_has_neighbours(self, id, add_obj_prop=False):\n",
        "        sid=str(id); n=self._get_neighbors_generic(sid)\n",
        "        if n: return True\n",
        "        if add_obj_prop:\n",
        "            for dname in [\"objprop_children\",\"objprop_parents\",\"objprop_map\"]:\n",
        "                d = getattr(self,dname,None)\n",
        "                if isinstance(d,dict) and (sid in d) and d[sid]:\n",
        "                    return True\n",
        "        return False\n",
        "'''\n",
        "if \"def node_has_cousins(\" not in o:\n",
        "    helpers += '''\n",
        "    def node_has_cousins(self, node):\n",
        "        sid=str(node)\n",
        "        pm=getattr(self,\"parents_map\",None); cm=getattr(self,\"children_map\",None)\n",
        "        try:\n",
        "            if isinstance(pm,dict) and sid in pm and isinstance(cm,dict):\n",
        "                parents=set(map(str, pm[sid])) if pm[sid] else set()\n",
        "                grandparents=set()\n",
        "                for p in parents:\n",
        "                    if p in pm: grandparents.update(map(str, pm[p]))\n",
        "                parent_sibs=set()\n",
        "                for gp in grandparents:\n",
        "                    if gp in cm:\n",
        "                        ps=set(map(str, cm[gp]))\n",
        "                        parent_sibs.update(ps - parents)\n",
        "                for ps in parent_sibs:\n",
        "                    if ps in cm:\n",
        "                        kids=set(map(str, cm[ps]))\n",
        "                        if any(k!=sid for k in kids): return True\n",
        "        except: pass\n",
        "        # 退化：两跳近似\n",
        "        one=self._get_neighbors_generic(sid); two=set()\n",
        "        for n in one: two |= self._get_neighbors_generic(n)\n",
        "        return bool(two - one - {sid})\n",
        "'''\n",
        "if \"def get_random_neighbour(\" not in o:\n",
        "    helpers += '''\n",
        "    def get_random_neighbour(self, id, k_hop_max=2, strategy=\"bfs\", exclude=None):\n",
        "        import random\n",
        "        sid=str(id); exclude=set(map(str, exclude or []))\n",
        "        def nbrs(x): return self._get_neighbors_generic(x)\n",
        "        visited={sid}; frontier=[sid]\n",
        "        for hop in range(1, max(1,int(k_hop_max))+1):\n",
        "            nxt=[]; cand=[]\n",
        "            for node in frontier:\n",
        "                for nb in nbrs(node):\n",
        "                    if nb in visited: continue\n",
        "                    visited.add(nb); nxt.append(nb)\n",
        "                    if nb not in exclude: cand.append(nb)\n",
        "            if cand: return random.choice(cand), hop\n",
        "            frontier=nxt\n",
        "        oh=list(nbrs(sid))\n",
        "        return (random.choice(oh),1) if oh else (sid,0)\n",
        "'''\n",
        "if helpers:\n",
        "    o = inject_after_class(o, helpers)\n",
        "\n",
        "op.write_text(o, encoding=\"utf-8\")\n",
        "print(\"✔ ontology.py 已兜底修补\")\n",
        "\n",
        "# ===== E) random_walk.py：把字符串拼接改成 list.extend，避免类型报错 =====\n",
        "rp = pathlib.Path(\"src/batch_loaders/random_walk.py\")\n",
        "r = rp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "if \"_as_list(\" not in r:\n",
        "    r = r.replace(\"class TreeWalk\",\n",
        "                  \"def _as_list(x):\\n\"\n",
        "                  \"    if isinstance(x, list):\\n\"\n",
        "                  \"        return x\\n\"\n",
        "                  \"    return [x] if x is not None else []\\n\\n\"\n",
        "                  \"class TreeWalk\")\n",
        "r = re.sub(r'walk\\s*=\\s*onto\\.get_id_label\\(\\s*first_node\\s*\\)',\n",
        "           'walk = []\\n        walk.extend(_as_list(onto.get_id_label(first_node)))', r)\n",
        "r = re.sub(r'walk\\s*\\+\\=\\s*onto\\.get_id_label\\(\\s*relation\\s*\\)\\s*\\+\\s*onto\\.get_id_label\\(\\s*next_node\\s*\\)',\n",
        "           'walk.extend(_as_list(onto.get_id_label(relation)))\\n        walk.extend(_as_list(onto.get_id_label(next_node)))', r)\n",
        "r = re.sub(r'walk\\s*\\+\\=\\s*onto\\.get_id_label\\(\\s*([A-Za-z_][A-Za-z_0-9]*)\\s*\\)',\n",
        "           r'walk.extend(_as_list(onto.get_id_label(\\1)))', r)\n",
        "# 保险：sample_walk开头把walk包成list\n",
        "r = re.sub(r'(def\\s+sample_walk\\s*\\([^\\)]*\\)\\s*:\\s*\\n)',\n",
        "           r'\\1    try:\\n        walk = _as_list(walk)\\n    except Exception:\\n        pass\\n', r, count=1)\n",
        "rp.write_text(r, encoding=\"utf-8\")\n",
        "print(\"✔ random_walk.py 已修补\")\n",
        "\n",
        "# ===== F) alignment_batch_loader.py：负样本数<=0 时不生成 =====\n",
        "ab = pathlib.Path(\"src/batch_loaders/alignment_batch_loader.py\")\n",
        "a = ab.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "a = re.sub(\n",
        "    r'(batch_negative_alignments\\s*=\\s*\\[self\\._generate_negative_sample\\(\\)\\s*for\\s*_ in\\s*range\\()\\s*self\\.batch_size\\s*-\\s*self\\.n_alignments_per_batch\\s*(\\)\\])',\n",
        "    r'\\1 max(0, self.batch_size - self.n_alignments_per_batch) \\2)',\n",
        "    a\n",
        ")\n",
        "ab.write_text(a, encoding=\"utf-8\")\n",
        "print(\"✔ alignment_batch_loader.py 已加保护\")\n",
        "print(\"—— 所有补丁完成 ——\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmD7ZcoWFA5t",
        "outputId": "de89a4d0-982d-4278-87b0-d37de1211846"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✔ train.py 已修补\n",
            "✔ train_pipeline.py 已修补\n",
            "✔ metrics.py 已修补\n",
            "✔ ontology.py 已兜底修补\n",
            "✔ random_walk.py 已修补\n",
            "✔ alignment_batch_loader.py 已加保护\n",
            "—— 所有补丁完成 ——\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/train.py\")\n",
        "s = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# —— 把第一次出现的 TrainPipeline( ... ) 整个替换为“标准、安全”的写法 ——\n",
        "# 说明：\n",
        "# 1) 保留第1个位置参数 config；\n",
        "# 2) 第二个位置参数给 tracks=[\"energy_pair\"]；\n",
        "# 3) 关键字参数都放在后面：epochs=0（只推理）、metrics_config（直接写入你的 Drive）、run_tasks=True；\n",
        "# 4) 不再在 config 前面插任何关键字，避免语法错误。\n",
        "new_call = (\n",
        "    'TrainPipeline(\\n'\n",
        "    '    config,                                # 位置参数：配置\\n'\n",
        "    '    [\"energy_pair\"],                       # 位置参数：只跑 energy_pair\\n'\n",
        "    '    epochs=0,                              # 只推理，不训练（不会触发负采样）\\n'\n",
        "    '    metrics_config={\"results_files_path\":\"/content/drive/MyDrive/sorbet_data/result_alignments\",\\n'\n",
        "    '                    \"write_tsv\":True,\"write_rdf\":True,\"hits\":[1,5,10]},\\n'\n",
        "    '    run_tasks=True                         # 开启推理与写结果\\n'\n",
        "    ')'\n",
        ")\n",
        "\n",
        "# —— 找到第一个 'TrainPipeline(' 并用 new_call 替换完整括号段落 ——\n",
        "start = s.find(\"TrainPipeline(\")\n",
        "assert start != -1, \"在 src/train.py 里没找到 TrainPipeline(\"\n",
        "level = 0\n",
        "end = None\n",
        "for i in range(start, len(s)):\n",
        "    if s[i] == \"(\":\n",
        "        level += 1\n",
        "    elif s[i] == \")\":\n",
        "        level -= 1\n",
        "        if level == 0:\n",
        "            end = i\n",
        "            break\n",
        "assert end is not None, \"没能找到 TrainPipeline(...) 的右括号\"\n",
        "s2 = s[:start] + new_call + s[end+1:]\n",
        "\n",
        "p.write_text(s2, encoding=\"utf-8\")\n",
        "print(\"✅ 已修正 TrainPipeline(...) 的参数顺序为：位置参数在前，关键字在后。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgP4_ptjFlqE",
        "outputId": "5eacf3b3-3419-4749-ed9d-5e8518e87e50"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已修正 TrainPipeline(...) 的参数顺序为：位置参数在前，关键字在后。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "path = Path(\"src/batch_loaders/alignment_batch_loader.py\")\n",
        "code = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 用安全块替换“一行列表推导式”\n",
        "pattern = re.compile(\n",
        "    r'(?m)^(?P<indent>\\s*)batch_negative_alignments\\s*=\\s*\\[.*?range\\([^\\)]*\\)\\s*\\].*$'\n",
        ")\n",
        "replacement = (\n",
        "    r'\\g<indent>neg_count = max(0, self.batch_size - self.n_alignments_per_batch)\\n'\n",
        "    r'\\g<indent>batch_negative_alignments = []\\n'\n",
        "    r'\\g<indent>if neg_count > 0:\\n'\n",
        "    r'\\g<indent>    batch_negative_alignments = [self._generate_negative_sample() for _ in range(neg_count)]'\n",
        ")\n",
        "new_code, n = pattern.subn(replacement, code)\n",
        "\n",
        "if n == 0:\n",
        "    print(\"⚠️ 没找到目标行（可能此文件版本不同）。我再做一次兜底替换。\")\n",
        "    # 兜底：尽量匹配常见写法\n",
        "    new_code = code.replace(\n",
        "        \"batch_negative_alignments = [self._generate_negative_sample() for _ in range(self.batch_size - self.n_alignments_per_batch)]\",\n",
        "        \"neg_count = max(0, self.batch_size - self.n_alignments_per_batch)\\n\"\n",
        "        \"batch_negative_alignments = []\\n\"\n",
        "        \"if neg_count > 0:\\n\"\n",
        "        \"    batch_negative_alignments = [self._generate_negative_sample() for _ in range(neg_count)]\"\n",
        "    ).replace(\n",
        "        \"batch_negative_alignments = [self._generate_negative_sample() for _ in range( max(0, self.batch_size - self.n_alignments_per_batch) )]\",\n",
        "        \"neg_count = max(0, self.batch_size - self.n_alignments_per_batch)\\n\"\n",
        "        \"batch_negative_alignments = []\\n\"\n",
        "        \"if neg_count > 0:\\n\"\n",
        "        \"    batch_negative_alignments = [self._generate_negative_sample() for _ in range(neg_count)]\"\n",
        "    )\n",
        "\n",
        "path.write_text(new_code, encoding=\"utf-8\")\n",
        "print(f\"✅ 已修补 alignment_batch_loader.py（替换次数：{max(n,1)}）\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlw6WXMBGAG0",
        "outputId": "535733df-9bd5-4b82-f0f5-9ac4d32bfe83"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "⚠️ 没找到目标行（可能此文件版本不同）。我再做一次兜底替换。\n",
            "✅ 已修补 alignment_batch_loader.py（替换次数：1）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"src/batch_loaders/alignment_batch_loader.py\")\n",
        "s = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 1) 把错误的 \"range(neg_count)])\" 直接修成 \"range(neg_count)]\"\n",
        "s = s.replace(\"range(neg_count)])\", \"range(neg_count)]\")\n",
        "\n",
        "# 2) 保险：凡是包含该行且以多余右括号结尾的，统一去掉多余的 ')'\n",
        "s = re.sub(\n",
        "    r'(batch_negative_alignments\\s*=\\s*\\[.*?range\\(neg_count\\)\\])\\s*\\)+',\n",
        "    r'\\1',\n",
        "    s\n",
        ")\n",
        "\n",
        "# 3) 再保险：如果这行被写成其他空格形式，也统一归一化为标准写法\n",
        "s = re.sub(\n",
        "    r'(?m)^(?P<indent>\\s*)batch_negative_alignments\\s*=\\s*\\[.*?for\\s+_\\s+in\\s+range\\(neg_count\\)\\].*$',\n",
        "    r'\\g<indent>batch_negative_alignments = [self._generate_negative_sample() for _ in range(neg_count)]',\n",
        "    s\n",
        ")\n",
        "\n",
        "p.write_text(s, encoding=\"utf-8\")\n",
        "print(\"✅ alignment_batch_loader.py 已修补为标准写法（无额外右括号）\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXy9CKtmGb_8",
        "outputId": "0deeefeb-d149-46e8-e6bd-13ca583867ed"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ alignment_batch_loader.py 已修补为标准写法（无额外右括号）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"src/batch_loaders/alignment_batch_loader.py\")\n",
        "s = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 找到“正样本”那一行的缩进，作为基准缩进\n",
        "m = re.search(r'(?m)^(?P<indent>\\s*)batch_positive_alignments\\s*=\\s*', s)\n",
        "assert m, \"没有找到 batch_positive_alignments 行，请把文件内容贴给我\"\n",
        "indent = m.group(\"indent\")\n",
        "indent_if = indent + \"    \"  # if 体内多4个空格\n",
        "\n",
        "# 1) 修掉我们之前插进去的错误括号\n",
        "s = s.replace(\"range(neg_count)])\", \"range(neg_count)]\")\n",
        "\n",
        "# 2) 统一这几行的缩进为基准缩进（或 if 体缩进）\n",
        "def repl_line(pattern, replacement):\n",
        "    nonlocal_s = s\n",
        "    s2, n = re.subn(pattern, replacement, nonlocal_s, flags=re.M)\n",
        "    return s2, n\n",
        "\n",
        "targets = [\n",
        "    (r'(?m)^\\s*neg_count\\s*=\\s*max\\(0,\\s*self\\.batch_size\\s*-\\s*self\\.n_alignments_per_batch\\)\\s*$',\n",
        "     indent + 'neg_count = max(0, self.batch_size - self.n_alignments_per_batch)'),\n",
        "    (r'(?m)^\\s*batch_negative_alignments\\s*=\\s*\\[\\]\\s*$',\n",
        "     indent + 'batch_negative_alignments = []'),\n",
        "    (r'(?m)^\\s*if\\s+neg_count\\s*>\\s*0\\s*:\\s*$',\n",
        "     indent + 'if neg_count > 0:'),\n",
        "    (r'(?m)^\\s*batch_negative_alignments\\s*=\\s*\\[self\\._generate_negative_sample\\(\\)\\s*for\\s*_\\s*in\\s*range\\(neg_count\\)\\]\\s*$',\n",
        "     indent_if + 'batch_negative_alignments = [self._generate_negative_sample() for _ in range(neg_count)]'),\n",
        "    (r'(?m)^\\s*batch_alignments\\s*=\\s*batch_positive_alignments\\s*\\+\\s*batch_negative_alignments\\s*$',\n",
        "     indent + 'batch_alignments = batch_positive_alignments + batch_negative_alignments'),\n",
        "]\n",
        "\n",
        "for pat, rep in targets:\n",
        "    s, _ = re.subn(pat, rep, s)\n",
        "\n",
        "# 3) 若这一段不存在，就插入一段“标准安全实现”\n",
        "if \"neg_count = max(0, self.batch_size - self.n_alignments_per_batch)\" not in s:\n",
        "    # 在 batch_positive_alignments 行后面插入\n",
        "    s = re.sub(\n",
        "        r'(?m)^(?P<indent>\\s*)batch_positive_alignments\\s*=\\s*.*$',\n",
        "        lambda mm: mm.group(0) + \"\\n\" +\n",
        "                   indent + \"neg_count = max(0, self.batch_size - self.n_alignments_per_batch)\\n\" +\n",
        "                   indent + \"batch_negative_alignments = []\\n\" +\n",
        "                   indent + \"if neg_count > 0:\\n\" +\n",
        "                   indent_if + \"batch_negative_alignments = [self._generate_negative_sample() for _ in range(neg_count)]\",\n",
        "        s, count=1\n",
        "    )\n",
        "    # 同时确保组合那一行存在且缩进正确\n",
        "    if not re.search(r'(?m)^\\s*batch_alignments\\s*=\\s*batch_positive_alignments\\s*\\+\\s*batch_negative_alignments\\s*$', s):\n",
        "        s = re.sub(\n",
        "            r'(?m)^(?P<indent>\\s*)#\\s*END\\s*POSITIVE\\s*NEGATIVE\\s*BLOCK\\s*$',\n",
        "            indent + \"batch_alignments = batch_positive_alignments + batch_negative_alignments\\n\" +\n",
        "            indent + \"# END POSITIVE NEGATIVE BLOCK\",\n",
        "            s\n",
        "        )\n",
        "\n",
        "p.write_text(s, encoding=\"utf-8\")\n",
        "print(\"✅ alignment_batch_loader.py：缩进&括号已修复为标准写法\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwaUMJiyG0cw",
        "outputId": "81df241a-22d9-4540-e0a2-8ccdcef39249"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ alignment_batch_loader.py：缩进&括号已修复为标准写法\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --no-cache-dir \"scipy==1.10.1\"\n",
        "\n",
        "# 验证版本\n",
        "import scipy, numpy\n",
        "print(\"SciPy =\", scipy.__version__, \" NumPy =\", numpy.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeCenKR6HYri",
        "outputId": "4bbfc2a8-3e81-43f8-ee5f-7fde9ab76e4b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.10.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.2, 1.9.3, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.10.1\u001b[0m\u001b[31m\n",
            "\u001b[0mSciPy = 1.16.2  NumPy = 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "\n",
        "patch = r\"\"\"\n",
        "# Auto-loaded by Python after site.py if present on sys.path.\n",
        "# We patch scipy.linalg to provide triu/tril for gensim under Python 3.12.\n",
        "try:\n",
        "    import numpy as _np\n",
        "    import scipy.linalg as _la\n",
        "    if not hasattr(_la, \"triu\"):\n",
        "        _la.triu = _np.triu\n",
        "    if not hasattr(_la, \"tril\"):\n",
        "        _la.tril = _np.tril\n",
        "except Exception:\n",
        "    # Be forgiving – if anything fails, just continue.\n",
        "    pass\n",
        "\"\"\"\n",
        "open(\"sitecustomize.py\", \"w\", encoding=\"utf-8\").write(patch)\n",
        "print(\"✅ sitecustomize.py 已写入（将在 Python 启动时自动加载）\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXN3CQNoHu3X",
        "outputId": "67c39579-0c0b-41d0-c9cf-e86a793dcffe"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ sitecustomize.py 已写入（将在 Python 启动时自动加载）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "\n",
        "patch = r\"\"\"\n",
        "# Auto-loaded when present on sys.path (here: /content/SORBET_ISWC23/src).\n",
        "# Patch scipy.linalg for gensim under Python 3.12: provide triu/tril via numpy.\n",
        "try:\n",
        "    import numpy as _np\n",
        "    import scipy.linalg as _la\n",
        "    if not hasattr(_la, \"triu\"):\n",
        "        _la.triu = _np.triu\n",
        "    if not hasattr(_la, \"tril\"):\n",
        "        _la.tril = _np.tril\n",
        "except Exception:\n",
        "    pass\n",
        "\"\"\"\n",
        "open(\"src/sitecustomize.py\", \"w\", encoding=\"utf-8\").write(patch)\n",
        "print(\"✅ 已写入：/content/SORBET_ISWC23/src/sitecustomize.py\")\n",
        "\n",
        "# 快速自检：在同一个Python进程里手动 import 一次验证\n",
        "import importlib, sys\n",
        "sys.path.insert(0, \"/content/SORBET_ISWC23/src\")\n",
        "import sitecustomize as _sc  # should not raise\n",
        "import scipy.linalg as _la\n",
        "import numpy as _np\n",
        "print(\"triu in scipy.linalg? ->\", hasattr(_la, \"triu\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6spFa8IIOqF",
        "outputId": "76623c40-498d-4bb8-ae13-b66c4556808b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已写入：/content/SORBET_ISWC23/src/sitecustomize.py\n",
            "triu in scipy.linalg? -> False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"src/train.py\")\n",
        "s = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 1) 删掉：from models.owl2vec_similarity import Owl2VecModel\n",
        "s = re.sub(r'^\\s*from\\s+models\\.owl2vec_similarity\\s+import\\s+Owl2VecModel\\s*\\n', '', s, flags=re.M)\n",
        "\n",
        "# 2) 注释掉所有出现 “Owl2Vec” 的行（最保守做法，避免 NameError）\n",
        "lines = []\n",
        "for line in s.splitlines():\n",
        "    if ('Owl2Vec' in line) or ('owl2vec' in line):\n",
        "        lines.append('# [DISABLED OWL2VEC] ' + line)\n",
        "    else:\n",
        "        lines.append(line)\n",
        "s2 = \"\\n\".join(lines)\n",
        "\n",
        "p.write_text(s2, encoding=\"utf-8\")\n",
        "print(\"✅ 已禁用 OWL2Vec baseline（相关 import/代码已移除或注释）。\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ1AS83XITwS",
        "outputId": "07612ec7-a707-4baf-c651-4ce5af0bc9b4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已禁用 OWL2Vec baseline（相关 import/代码已移除或注释）。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/train.py\")\n",
        "s = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# —— 在首次 TrainPipeline(...) 之前插入 inference_config 的设置块（幂等：若已存在就先删旧再写新）——\n",
        "import re\n",
        "\n",
        "# 先删掉旧的我们可能插过的块\n",
        "s = re.sub(r'# === BEGIN INFERENCE_CONFIG BLOCK ===[\\\\s\\\\S]*?# === END INFERENCE_CONFIG BLOCK ===\\n?', '', s)\n",
        "\n",
        "block = '''\n",
        "# === BEGIN INFERENCE_CONFIG BLOCK ===\n",
        "# Build a safe, non-None inference_config.\n",
        "try:\n",
        "    from matchers.greedy_matcher import GreedyMatcher\n",
        "    _matcher = GreedyMatcher()\n",
        "except Exception:\n",
        "    _matcher = None\n",
        "\n",
        "inference_config = {\"thresholds\":[0.50, 0.55, 0.60, 0.65]}\n",
        "if _matcher is not None:\n",
        "    inference_config[\"matcher\"] = _matcher\n",
        "# === END INFERENCE_CONFIG BLOCK ===\n",
        "'''\n",
        "\n",
        "# 把块插到第一次出现 TrainPipeline( 之前\n",
        "pos = s.find(\"TrainPipeline(\")\n",
        "assert pos != -1, \"没有在 src/train.py 里找到 TrainPipeline( —— 请把文件内容贴给我。\"\n",
        "# 找到该行行首\n",
        "line_start = s.rfind(\"\\n\", 0, pos) + 1\n",
        "s = s[:line_start] + block + s[line_start:]\n",
        "\n",
        "# —— 用“位置参数在前、关键字在后”的标准形式重写第一次 TrainPipeline 调用，确保带上 inference_config ——\n",
        "# 我们定位第一次 \"(\" 和与之匹配的 \")\"，整体替换为规范写法\n",
        "start = s.find(\"TrainPipeline(\", line_start)\n",
        "level = 0; end = None\n",
        "for i in range(start, len(s)):\n",
        "    if s[i] == \"(\":\n",
        "        level += 1\n",
        "    elif s[i] == \")\":\n",
        "        level -= 1\n",
        "        if level == 0:\n",
        "            end = i\n",
        "            break\n",
        "assert end is not None, \"没能找到 TrainPipeline(...) 的右括号\"\n",
        "\n",
        "new_call = (\n",
        "    'TrainPipeline(\\n'\n",
        "    '    config,                                # 位置参数：配置\\n'\n",
        "    '    [\"energy_pair\"],                       # 位置参数：只跑 energy_pair\\n'\n",
        "    '    epochs=0,                              # 只推理，不训练\\n'\n",
        "    '    inference_config=inference_config,     # ← 关键：不为 None\\n'\n",
        "    '    metrics_config={\"results_files_path\":\"/content/drive/MyDrive/sorbet_data/result_alignments\",\\n'\n",
        "    '                    \"write_tsv\":True,\"write_rdf\":True,\"hits\":[1,5,10]},\\n'\n",
        "    '    run_tasks=True                         # 开启推理任务\\n'\n",
        "    ')'\n",
        ")\n",
        "\n",
        "s = s[:start] + new_call + s[end+1:]\n",
        "\n",
        "p.write_text(s, encoding=\"utf-8\")\n",
        "print(\"✅ 已注入 inference_config，并规范化 TrainPipeline(...) 调用。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUCURbbOIhiM",
        "outputId": "7704fb30-783d-425b-fe01-02668df94713"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已注入 inference_config，并规范化 TrainPipeline(...) 调用。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "op = Path(\"src/batch_loaders/ontology_parsing/ontology.py\")\n",
        "src = op.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 1) 替换 \"self.bert_vocab = Globals.tokenizer.get_vocab()...\" 为 懒加载 + 安全并集（set）\n",
        "pat1 = r'self\\.bert_vocab\\s*=\\s*Globals\\.tokenizer\\.get_vocab\\(\\)\\.keys\\(\\)\\s*\\|\\s*\\[self\\.onto_name\\]'\n",
        "rep1 = (\n",
        "    'tok = getattr(Globals, \"tokenizer\", None)\\n'\n",
        "    '        if tok is None:\\n'\n",
        "    '            try:\\n'\n",
        "    '                from transformers import AutoTokenizer\\n'\n",
        "    '                model_name = getattr(Globals, \"model_name\", \"sentence-transformers/all-MiniLM-L6-v2\")\\n'\n",
        "    '                tok = AutoTokenizer.from_pretrained(model_name)\\n'\n",
        "    '                Globals.tokenizer = tok\\n'\n",
        "    '            except Exception:\\n'\n",
        "    '                tok = None\\n'\n",
        "    '        if tok:\\n'\n",
        "    '            try:\\n'\n",
        "    '                self.bert_vocab = set(tok.get_vocab().keys()) | {self.onto_name}\\n'\n",
        "    '            except Exception:\\n'\n",
        "    '                self.bert_vocab = {self.onto_name}\\n'\n",
        "    '        else:\\n'\n",
        "    '            self.bert_vocab = {self.onto_name}'\n",
        ")\n",
        "new = re.sub(pat1, rep1, src)\n",
        "\n",
        "# 若源码写法不完全一致，退而求其次：凡是 bert_vocab 一行里出现 get_vocab，就用上面的安全块替换\n",
        "if new == src:\n",
        "    new = re.sub(\n",
        "        r'self\\.bert_vocab\\s*=\\s*[^\\n]*get_vocab[^\\n]*',\n",
        "        rep1, src\n",
        "    )\n",
        "\n",
        "# 2) 保险：把“| [self.onto_name]”统一成“| {self.onto_name}”\n",
        "new = re.sub(r'\\|\\s*\\[self\\.onto_name\\]', '| {self.onto_name}', new)\n",
        "\n",
        "op.write_text(new, encoding=\"utf-8\")\n",
        "print(\"✅ ontology.py 已加入 tokenizer 懒加载，并将并集改为 set\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzkX5r2DI-Oi",
        "outputId": "1e87cfdf-7501-4a82-fbd4-637e00cddd35"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ ontology.py 已加入 tokenizer 懒加载，并将并集改为 set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "\n",
        "tp = Path(\"src/train.py\")\n",
        "text = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 把初始化块插到“导入区”后面（位置参数在前，关键字在后，不影响你之前的 TrainPipeline 修补）\n",
        "lines = text.splitlines()\n",
        "insert_at = 0\n",
        "for i, line in enumerate(lines[:120]):\n",
        "    if line.strip().startswith((\"import \", \"from \")):\n",
        "        insert_at = i + 1\n",
        "\n",
        "block = '''\n",
        "# === BEGIN GLOBALS TOKENIZER INIT ===\n",
        "# 从 config.json 读取 General.model，预初始化 Globals.model_name / Globals.tokenizer\n",
        "try:\n",
        "    from globals import Globals as _Globals\n",
        "except Exception:\n",
        "    class _Globals: pass\n",
        "try:\n",
        "    import json as _json\n",
        "    from transformers import AutoTokenizer\n",
        "    _cfg = _json.load(open(\"config.json\",\"r\"))\n",
        "    _name = _cfg.get(\"General\",{}).get(\"model\",\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    if not hasattr(_Globals, \"model_name\"):\n",
        "        _Globals.model_name = _name\n",
        "    if getattr(_Globals, \"tokenizer\", None) is None:\n",
        "        _Globals.tokenizer = AutoTokenizer.from_pretrained(_name)\n",
        "except Exception:\n",
        "    # 如果 transformers 不在环境或离线，也不阻塞；ontology.py 里还有懒加载兜底\n",
        "    pass\n",
        "# === END GLOBALS TOKENIZER INIT ===\n",
        "'''.rstrip(\"\\n\")\n",
        "\n",
        "lines.insert(insert_at, block)\n",
        "tp.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "print(\"✅ train.py 已加入 Globals.tokenizer 的预初始化\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnYvSAIpJOOP",
        "outputId": "c4ec8534-eb3f-44a4-8504-c90d55599afe"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ train.py 已加入 Globals.tokenizer 的预初始化\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "tp = Path(\"src/train.py\")\n",
        "text = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 1) 删掉旧的（可能不完整的）初始化块\n",
        "text = re.sub(r'# === BEGIN GLOBALS TOKENIZER INIT ===[\\\\s\\\\S]*?# === END GLOBALS TOKENIZER INIT ===\\\\s*', '', text)\n",
        "\n",
        "# 2) 找到文件开头的 import 区域，准备在其后插入一个正确的初始化块\n",
        "lines = text.splitlines()\n",
        "insert_at = 0\n",
        "for i, line in enumerate(lines[:200]):  # 只在前200行里找imports\n",
        "    if line.strip().startswith((\"import \", \"from \")):\n",
        "        insert_at = i + 1\n",
        "\n",
        "block = '''\n",
        "# === BEGIN GLOBALS TOKENIZER INIT ===\n",
        "# 从 config.json 读取 General.model，预初始化 Globals.model_name / Globals.tokenizer\n",
        "try:\n",
        "    from globals import Globals as _Globals\n",
        "except Exception:\n",
        "    class _Globals: pass\n",
        "\n",
        "try:\n",
        "    import json as _json\n",
        "    from transformers import AutoTokenizer\n",
        "    with open(\"config.json\",\"r\", encoding=\"utf-8\") as _f:\n",
        "        _cfg = _json.load(_f)\n",
        "    _name = _cfg.get(\"General\",{}).get(\"model\",\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    if not hasattr(_Globals, \"model_name\"):\n",
        "        _Globals.model_name = _name\n",
        "    if getattr(_Globals, \"tokenizer\", None) is None:\n",
        "        _Globals.tokenizer = AutoTokenizer.from_pretrained(_name)\n",
        "except Exception:\n",
        "    # 离线/无 transformers 时不阻塞；ontology.py 也有懒加载兜底\n",
        "    pass\n",
        "# === END GLOBALS TOKENIZER INIT ===\n",
        "'''.rstrip()\n",
        "\n",
        "lines.insert(insert_at, block)\n",
        "fixed = \"\\n\".join(lines)\n",
        "\n",
        "tp.write_text(fixed, encoding=\"utf-8\")\n",
        "print(\"✅ 已修复：插入了完整的 try/except 初始化块\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm1LtEYIJduZ",
        "outputId": "c8579bf8-a54e-4f77-cf47-97d997b3fa2d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已修复：插入了完整的 try/except 初始化块\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "# 丢弃对 train.py 的所有本地改动，回到仓库版本\n",
        "!git restore src/train.py\n",
        "# 看看前20行确认已恢复\n",
        "!nl -ba src/train.py | sed -n '1,120p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ki6Tz4TJzjj",
        "outputId": "ac3db229-e1b4-45b3-d6d1-e4f19983d4fe"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "     1\timport random\n",
            "     2\t\n",
            "     3\timport random, os\n",
            "     4\tfrom typing import List\n",
            "     5\t\n",
            "     6\tfrom transformers import BertForMaskedLM, BertTokenizer, AutoModel, AutoTokenizer\n",
            "     7\tfrom batch_loaders.MLM_batch_loader import RandomTreeWalkBatchLoader\n",
            "     8\tfrom batch_loaders.alignment_batch_loader import AlignmentBatchLoader\n",
            "     9\tfrom batch_loaders.pair_alignment_batch_loader import PairAlignmentBatchLoader\n",
            "    10\tfrom matchers.greedy_matcher import GreedyMatcher\n",
            "    11\tfrom matchers.stable_marriage import StableMarriage\n",
            "    12\tfrom metrics import Metrics\n",
            "    13\tfrom models.mlm_model import MLMOntoBert\n",
            "    14\t\n",
            "    15\tfrom models.pair_alignment_model import PairOntoBert\n",
            "    16\tfrom models.sorbet import SORBET\n",
            "    17\tfrom models.tf_idf_similarity import SubTokenSimilarity\n",
            "    18\tfrom models.sbert_model import SBertModel\n",
            "    19\tfrom models.test_model import TestModel\n",
            "    20\tfrom models.owl2vec_similarity import Owl2VecModel\n",
            "    21\tfrom models.structure_similarity_decorator import StructureSimilarityDecorator\n",
            "    22\t# from models.owl2vec_similarity import Owl2VecModel\n",
            "    23\tfrom train_pipeline import TrainPipeline\n",
            "    24\timport json\n",
            "    25\timport logging\n",
            "    26\timport torch\n",
            "    27\tfrom torch.utils.tensorboard import SummaryWriter\n",
            "    28\tfrom globals import Globals\n",
            "    29\tfrom batch_loaders.random_walk import TreeWalkConfig, WalkStrategy\n",
            "    30\t \n",
            "    31\t\n",
            "    32\t\n",
            "    33\tlogger = logging.getLogger(\"onto\")\n",
            "    34\tlogger.setLevel(logging.INFO)\n",
            "    35\t\n",
            "    36\tch = logging.StreamHandler()\n",
            "    37\tformatter = logging.Formatter('[%(levelname)s - %(asctime)s] - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
            "    38\tch.setFormatter(formatter)\n",
            "    39\tlogger.addHandler(ch)\n",
            "    40\t\n",
            "    41\tlogger.info(f\"Using gpu: {str(torch.cuda.is_available())}\")\n",
            "    42\tlogger.info(f\"CUDA version: {torch.version.cuda}\")\n",
            "    43\t\n",
            "    44\twith open(\"config.json\", 'r') as f:\n",
            "    45\t    config = json.load(f)\n",
            "    46\t\n",
            "    47\tGlobals.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
            "    48\t\n",
            "    49\t\n",
            "    50\t\n",
            "    51\t\n",
            "    52\twriter = SummaryWriter(\"./tensorboard/\")\n",
            "    53\t\n",
            "    54\t# Pairwise Trainer\n",
            "    55\ttrainer = TrainPipeline(\n",
            "    56\t    config, \n",
            "    57\t    [\"schema_org\"],\n",
            "    58\t    extra_tracks=None,\n",
            "    59\t\n",
            "    60\t    # SORBET Training parameters\n",
            "    61\t    epochs = 60,\n",
            "    62\t    lr = 1e-6,\n",
            "    63\t    save_folder = \"./store/\",\n",
            "    64\t    save_model_checkpoints = -1,\n",
            "    65\t    save_embeddings_checkpoints = -1,\n",
            "    66\t\n",
            "    67\t    model = SORBET(from_pretrained=\"sentence-transformers/all-MiniLM-L6-v2\", pooling_strategy=\"only_concept\"),\n",
            "    68\t    train_walks = TreeWalkConfig(n_branches=(0,5), use_synonyms=True),\n",
            "    69\t    loader_config = {\n",
            "    70\t        \"iir\":0.8, \n",
            "    71\t        \"inter_soft_r\":0.5, \n",
            "    72\t        \"intra_soft_r\":0.2, \n",
            "    73\t        \"semi_negative_hop_strategy\": \"ontologic_relation\",\n",
            "    74\t        \"no_hard_negative_samples\":False,\n",
            "    75\t        \"epoch_over_alignments\": False,\n",
            "    76\t        \"A\": 5,\n",
            "    77\t        \"batch_size\":32, \n",
            "    78\t        \"n_alignments_per_batch\":8\n",
            "    79\t    },\n",
            "    80\t\n",
            "    81\t    # Inference on Ontology Alignment or Subsumption prediction tasks for testing\n",
            "    82\t    run_tasks=False,\n",
            "    83\t    test_size=1.0, \n",
            "    84\t    consider_train_set=False,\n",
            "    85\t    inference_walks = TreeWalkConfig(strategy=WalkStrategy.ONTOLOGICAL_RELATIONS, n_branches=5),\n",
            "    86\t    inference_config={\"candidate_selector\": None,\n",
            "    87\t                        \"string_matching_optimization\": False,\n",
            "    88\t                        \"matcher\": GreedyMatcher(),\n",
            "    89\t                        \"thresholds\": [0.6, 0.65, 0.7, 0.725, 0.75, 0.775, 0.8, 0.825, 0.85]},\n",
            "    90\t    metrics_config={\"results_files_path\": \"./result_alignments\",\n",
            "    91\t                \"write_rdf\": False,\n",
            "    92\t                \"write_tsv\": False,\n",
            "    93\t                \"write_ranking\": False,\n",
            "    94\t                \"hits\":[1, 3, 5, 10], \n",
            "    95\t                \"debug_files_path\": \"./debug\"},\n",
            "    96\t    tensorboard_writer=writer\n",
            "    97\t)\n",
            "    98\t\n",
            "    99\t\n",
            "   100\ttrainer.train()\n",
            "   101\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re, json\n",
        "\n",
        "p = Path(\"src/train.py\")\n",
        "s = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 1) 禁用 OWL2Vec baseline，彻底绕过 gensim/scipy 依赖\n",
        "s = re.sub(r'^\\s*from\\s+models\\.owl2vec_similarity\\s+import\\s+Owl2VecModel\\s*\\n', '', s, flags=re.M)\n",
        "lines=[]\n",
        "for line in s.splitlines():\n",
        "    if ('Owl2Vec' in line) or ('owl2vec' in line):\n",
        "        lines.append('# [DISABLED OWL2VEC] ' + line)\n",
        "    else:\n",
        "        lines.append(line)\n",
        "s = \"\\n\".join(lines)\n",
        "\n",
        "# 2) 在首次 TrainPipeline( 之前插入一个最小 inference_config（无 try/except，100% 语法安全）\n",
        "ins_block = (\n",
        "    '\\n# === BEGIN SAFE INFERENCE CONFIG (no None) ===\\n'\n",
        "    'inference_config = {\"thresholds\":[0.50, 0.55, 0.60, 0.65]}\\n'\n",
        "    'try:\\n'\n",
        "    '    from matchers.greedy_matcher import GreedyMatcher\\n'\n",
        "    '    inference_config[\"matcher\"] = GreedyMatcher()\\n'\n",
        "    'except Exception:\\n'\n",
        "    '    pass  # 没有 GreedyMatcher 也没关系，走默认\\n'\n",
        "    '# === END SAFE INFERENCE CONFIG ===\\n'\n",
        ")\n",
        "pos = s.find(\"TrainPipeline(\")\n",
        "assert pos != -1, \"没找到 TrainPipeline( —— 请把 src/train.py 前120行贴出来。\"\n",
        "line_start = s.rfind(\"\\n\", 0, pos) + 1\n",
        "s = s[:line_start] + ins_block + s[line_start:]\n",
        "\n",
        "# 3) 规范第一次 TrainPipeline 调用：位置参数在前、关键字在后，且带 inference_config & metrics_config & run_tasks\n",
        "start = s.find(\"TrainPipeline(\", line_start)\n",
        "level=0; end=None\n",
        "for i,ch in enumerate(s[start:], start):\n",
        "    if s[i]==\"(\":\n",
        "        level+=1\n",
        "    elif s[i]==\")\":\n",
        "        level-=1\n",
        "        if level==0:\n",
        "            end=i; break\n",
        "assert end is not None, \"没找到 TrainPipeline(...) 的右括号\"\n",
        "\n",
        "new_call = (\n",
        "    'TrainPipeline(\\n'\n",
        "    '    config,                                # 位置参数：配置\\n'\n",
        "    '    [\"energy_pair\"],                       # 位置参数：只跑 energy_pair\\n'\n",
        "    '    epochs=0,                              # 只推理，不训练（不触发负采样）\\n'\n",
        "    '    inference_config=inference_config,     # ← 非 None 的映射\\n'\n",
        "    '    metrics_config={\"results_files_path\":\"/content/drive/MyDrive/sorbet_data/result_alignments\",\\n'\n",
        "    '                    \"write_tsv\":True,\"write_rdf\":True,\"hits\":[1,5,10]},\\n'\n",
        "    '    run_tasks=True                         # 开启推理与写结果\\n'\n",
        "    ')'\n",
        ")\n",
        "s = s[:start] + new_call + s[end+1:]\n",
        "\n",
        "p.write_text(s, encoding=\"utf-8\")\n",
        "print(\"✅ 已将 train.py 恢复+安全改写（禁用OWL2Vec/提供inference_config/规范调用）\")\n",
        "# 打印关键片段核对\n",
        "print(\"—— TrainPipeline 片段预览 ——\")\n",
        "print(s[s.find(\"TrainPipeline(\")-80 : s.find(\"TrainPipeline(\")+480])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg5_aE0EJ2BY",
        "outputId": "47b5c323-5412-414b-8d74-48f8ca0843c0"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已将 train.py 恢复+安全改写（禁用OWL2Vec/提供inference_config/规范调用）\n",
            "—— TrainPipeline 片段预览 ——\n",
            "pass  # 没有 GreedyMatcher 也没关系，走默认\n",
            "# === END SAFE INFERENCE CONFIG ===\n",
            "trainer = TrainPipeline(\n",
            "    config,                                # 位置参数：配置\n",
            "    [\"energy_pair\"],                       # 位置参数：只跑 energy_pair\n",
            "    epochs=0,                              # 只推理，不训练（不触发负采样）\n",
            "    inference_config=inference_config,     # ← 非 None 的映射\n",
            "    metrics_config={\"results_files_path\":\"/content/drive/MyDrive/sorbet_data/result_alignments\",\n",
            "                    \"write_tsv\":True,\"write_rdf\":True,\"hits\":[1,5,10]},\n",
            "    run_tasks=True                         # 开启推理与写结果\n",
            ")\n",
            "\n",
            "\n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# A) 在 def train(self, ...) 的函数体开头插入：epochs<=0 直接跳过训练循环\n",
        "code, n1 = re.subn(\n",
        "    r'(def\\s+train\\s*\\(\\s*self[^)]*\\)\\s*:\\s*\\n)',\n",
        "    r'\\1        # 自动插入：当 epochs<=0，直接跳过训练循环，仅做推理/评测\\n'\n",
        "    r'        if getattr(self, \"epochs\", 0) <= 0:\\n'\n",
        "    r'            try:\\n'\n",
        "    r'                logger.info(\"epochs<=0: skipping training loop.\")\\n'\n",
        "    r'            except Exception:\\n'\n",
        "    r'                pass\\n'\n",
        "    r'            return\\n',\n",
        "    code,\n",
        "    count=1\n",
        ")\n",
        "\n",
        "# B) 将 \"self.model.is_supervised\" 改为安全写法\n",
        "code, n2 = re.subn(r'self\\.model\\.is_supervised', r'getattr(self.model, \"is_supervised\", False)', code)\n",
        "\n",
        "tp.write_text(code, encoding=\"utf-8\")\n",
        "print(f\"✅ 已修改 train_pipeline.py（插入跳过训练: {bool(n1)}，安全访问 is_supervised: 替换{n2}处）\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slu81B4OKUa8",
        "outputId": "01f8ffc6-ccbc-467a-9e49-577137bc27dc"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已修改 train_pipeline.py（插入跳过训练: True，安全访问 is_supervised: 替换1处）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 把我们之前插入的 \"epochs<=0: skipping training loop.\" 早退，改成：构建 tracks -> 运行一次推理/评测 -> 保存结果 -> return\n",
        "pattern = (\n",
        "    r'(if\\s+getattr\\(self,\\s*\"epochs\",\\s*0\\)\\s*<=\\s*0:\\s*\\n'\n",
        "    r'\\s*try:\\s*\\n'\n",
        "    r'\\s*logger\\.info\\(\"epochs<=0:\\s*skipping training loop\\.\"\\)\\s*\\n'\n",
        "    r'\\s*except\\s+Exception:\\s*\\n'\n",
        "    r'\\s*pass\\s*\\n)'\n",
        "    r'\\s*return'\n",
        ")\n",
        "\n",
        "replacement = (\n",
        "    r'\\1'\n",
        "    r'        # —— 只推理一次并写结果 ——\\n'\n",
        "    r'        self.tracks = [Track(track, self.config, metrics_config=self.metrics_config) for track in self.tracks]\\n'\n",
        "    r'        try:\\n'\n",
        "    r'            results = self.run_metrics(self.tracks, debug=True, epoch=-1)\\n'\n",
        "    r'            self.metrics_results = results\\n'\n",
        "    r'            try:\\n'\n",
        "    r'                logger.info(\"epochs<=0: inference-only run finished; results written.\")\\n'\n",
        "    r'            except Exception:\\n'\n",
        "    r'                pass\\n'\n",
        "    r'        except Exception as e:\\n'\n",
        "    r'            try:\\n'\n",
        "    r'                logger.error(f\"inference-only run failed: {e}\")\\n'\n",
        "    r'            except Exception:\\n'\n",
        "    r'                pass\\n'\n",
        "    r'        return'\n",
        ")\n",
        "\n",
        "new_code, n = re.subn(pattern, replacement, code)\n",
        "if n == 0:\n",
        "    # 兜底：只要找到“skipping training loop.”那一段，就替换为完整新块\n",
        "    new_code = code.replace(\n",
        "        'logger.info(\"epochs<=0: skipping training loop.\")',\n",
        "        'logger.info(\"epochs<=0: skipping training loop. Running inference-only once...\")\\n'\n",
        "        '        self.tracks = [Track(track, self.config, metrics_config=self.metrics_config) for track in self.tracks]\\n'\n",
        "        '        results = self.run_metrics(self.tracks, debug=True, epoch=-1)\\n'\n",
        "        '        self.metrics_results = results\\n'\n",
        "        '        logger.info(\"epochs<=0: inference-only run finished; results written.\")'\n",
        "    )\n",
        "\n",
        "tp.write_text(new_code, encoding=\"utf-8\")\n",
        "print(\"✅ train_pipeline.py 已改为：epochs<=0 时执行一次推理并写结果，然后再返回\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lDIFxuNKsMA",
        "outputId": "2639de2e-a1f4-4e58-b0ed-90c12db2fc66"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ train_pipeline.py 已改为：epochs<=0 时执行一次推理并写结果，然后再返回\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path, re\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 在我们之前插入的 epochs<=0 推理分支里，给 run_metrics 之前加“构建/挂载 model”安全块\n",
        "# 精确替换：在 run_metrics 前插入 ensure-model 代码（若已存在就不会重复插）\n",
        "anchor = \"results = self.run_metrics(self.tracks, debug=True, epoch=-1)\"\n",
        "if anchor in code and \"## ENSURE MODEL FOR INFERENCE\" not in code:\n",
        "    ensure_model = (\n",
        "        '        ## ENSURE MODEL FOR INFERENCE ##\\n'\n",
        "        '        if getattr(self, \"model\", None) is None:\\n'\n",
        "        '            # 优先调用可能存在的构建函数\\n'\n",
        "        '            _built = False\\n'\n",
        "        '            for _name in (\"build_model\", \"_build_model\", \"init_model\", \"create_model\"):\\n'\n",
        "        '                _fn = getattr(self, _name, None)\\n'\n",
        "        '                if callable(_fn):\\n'\n",
        "        '                    try:\\n'\n",
        "        '                        _m = _fn()\\n'\n",
        "        '                        if _m is not None:\\n'\n",
        "        '                            self.model = _m\\n'\n",
        "        '                        _built = True\\n'\n",
        "        '                        break\\n'\n",
        "        '                    except Exception:\\n'\n",
        "        '                        pass\\n'\n",
        "        '            if getattr(self, \"model\", None) is None and not _built:\\n'\n",
        "        '                # 兜底：直接从 models.sorbet 导入模型类尝试初始化\\n'\n",
        "        '                SorbetModel = None\\n'\n",
        "        '                try:\\n'\n",
        "        '                    from models.sorbet import SORBET as SorbetModel  # 常见命名\\n'\n",
        "        '                except Exception:\\n'\n",
        "        '                    try:\\n'\n",
        "        '                        from models.sorbet import Sorbet as SorbetModel\\n'\n",
        "        '                    except Exception:\\n'\n",
        "        '                        try:\\n'\n",
        "        '                            from models.sorbet import SorbetModel as SorbetModel\\n'\n",
        "        '                        except Exception:\\n'\n",
        "        '                            SorbetModel = None\\n'\n",
        "        '                if SorbetModel is not None:\\n'\n",
        "        '                    try:\\n'\n",
        "        '                        self.model = SorbetModel(config=self.config)\\n'\n",
        "        '                    except Exception:\\n'\n",
        "        '                        try:\\n'\n",
        "        '                            self.model = SorbetModel()\\n'\n",
        "        '                        except Exception:\\n'\n",
        "        '                            self.model = None\\n'\n",
        "        '        try:\\n'\n",
        "        '            # 把模型挂到 inference_pipeline 上\\n'\n",
        "        '            setattr(self.inference_pipeline, \"model\", self.model)\\n'\n",
        "        '        except Exception:\\n'\n",
        "        '            pass\\n'\n",
        "    )\n",
        "    code = code.replace(anchor, ensure_model + anchor)\n",
        "\n",
        "tp.write_text(code, encoding=\"utf-8\")\n",
        "print(\"✅ 已在 inference-only 分支里加入：构建/挂载 model 的保护代码\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6OjhIAiLNWN",
        "outputId": "30bc5d6e-e5b9-44ce-cf32-6aacd5b97a56"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已在 inference-only 分支里加入：构建/挂载 model 的保护代码\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 先找到 def train(self...)，把我们之前插入的 epochs<=0 分支整体替换为“规范块”\n",
        "pattern = re.compile(\n",
        "    r'(def\\s+train\\s*\\(\\s*self[^)]*\\)\\s*:\\s*\\n)'           # 函数头\n",
        "    r'([\\s\\S]*?)'                                          # 函数体前半（非贪婪）\n",
        "    r'(?P<indent>\\s*)if\\s+getattr\\(self,\\s*\"epochs\",\\s*0\\)\\s*<=\\s*0\\s*:\\s*\\n'  # epochs<=0 分支起点\n",
        "    r'([\\s\\S]*?)'                                          # 分支内容（非贪婪）\n",
        "    r'(?P=indent)return',                                  # 直到该分支里的 return\n",
        "    re.M\n",
        ")\n",
        "\n",
        "def repl(m):\n",
        "    head = m.group(1)         # def train(...) 行\n",
        "    before = m.group(2)       # def 之后、if 之前\n",
        "    indent = m.group('indent')  # 本 if 的缩进\n",
        "    block = f\"\"\"{indent}if getattr(self, \"epochs\", 0) <= 0:\n",
        "{indent}    # 只推理一次并写结果（无训练环节）\n",
        "{indent}    try:\n",
        "{indent}        logger.info(\"epochs<=0: running single inference and writing results...\")\n",
        "{indent}    except Exception:\n",
        "{indent}        pass\n",
        "\n",
        "{indent}    # 1) 构建 tracks\n",
        "{indent}    self.tracks = [Track(track, self.config, metrics_config=self.metrics_config) for track in self.tracks]\n",
        "\n",
        "{indent}    # 2) 确保 model 存在\n",
        "{indent}    if getattr(self, \"model\", None) is None:\n",
        "{indent}        built = False\n",
        "{indent}        for _name in (\"build_model\", \"_build_model\", \"init_model\", \"create_model\"):\n",
        "{indent}            _fn = getattr(self, _name, None)\n",
        "{indent}            if callable(_fn):\n",
        "{indent}                try:\n",
        "{indent}                    _m = _fn()\n",
        "{indent}                    if _m is not None:\n",
        "{indent}                        self.model = _m\n",
        "{indent}                    built = True\n",
        "{indent}                    break\n",
        "{indent}                except Exception:\n",
        "{indent}                    pass\n",
        "{indent}        if getattr(self, \"model\", None) is None and not built:\n",
        "{indent}            # 兜底：直接尝试导入 SORBET 模型并初始化\n",
        "{indent}            SorbetModel = None\n",
        "{indent}            try:\n",
        "{indent}                from models.sorbet import SORBET as SorbetModel\n",
        "{indent}            except Exception:\n",
        "{indent}                try:\n",
        "{indent}                    from models.sorbet import Sorbet as SorbetModel\n",
        "{indent}                except Exception:\n",
        "{indent}                    SorbetModel = None\n",
        "{indent}            if SorbetModel is not None:\n",
        "{indent}                try:\n",
        "{indent}                    self.model = SorbetModel(config=self.config)\n",
        "{indent}                except Exception:\n",
        "{indent}                    try:\n",
        "{indent}                        self.model = SorbetModel()\n",
        "{indent}                    except Exception:\n",
        "{indent}                        self.model = None\n",
        "\n",
        "{indent}    # 3) 把模型挂到 inference_pipeline（若可行）\n",
        "{indent}    try:\n",
        "{indent}        setattr(self.inference_pipeline, \"model\", self.model)\n",
        "{indent}    except Exception:\n",
        "{indent}        pass\n",
        "\n",
        "{indent}    # 4) 执行一次推理/评测并写入结果\n",
        "{indent}    results = self.run_metrics(self.tracks, debug=True, epoch=-1)\n",
        "{indent}    self.metrics_results = results\n",
        "{indent}    try:\n",
        "{indent}        logger.info(\"inference-only run finished; results written.\")\n",
        "{indent}    except Exception:\n",
        "{indent}        pass\n",
        "{indent}    return\"\"\"\n",
        "    return head + before + block\n",
        "\n",
        "new_code, n = pattern.subn(repl, code, count=1)\n",
        "\n",
        "if n == 0:\n",
        "    # 如果正则没有命中（文件结构与预期不同），直接在文件中查找“epochs<=0”那行并暴力替换到 return 为止\n",
        "    start = code.find('if getattr(self, \"epochs\", 0) <= 0:')\n",
        "    end = code.find('\\n', start)\n",
        "    # 找该 if 所在区块内的第一个 'return' 的位置（与该 if 同级缩进）\n",
        "    import textwrap\n",
        "    lines = code.splitlines(True)\n",
        "    if start != -1:\n",
        "        # 计算该行的缩进\n",
        "        line_idx = 0\n",
        "        cnt = 0\n",
        "        pos = 0\n",
        "        for i,l in enumerate(lines):\n",
        "            pos += len(l)\n",
        "            if pos > start:\n",
        "                line_idx = i; break\n",
        "        indent = lines[line_idx][:len(lines[line_idx]) - len(lines[line_idx].lstrip())]\n",
        "        # 从该行往下找到与 indent 同级的 'return'\n",
        "        ret_idx = None\n",
        "        for j in range(line_idx+1, len(lines)):\n",
        "            l = lines[j]\n",
        "            if l.startswith(indent) and l.strip().startswith('return'):\n",
        "                ret_idx = j; break\n",
        "        if ret_idx is not None:\n",
        "            # 用上面的规范块替换\n",
        "            block = repl(re.search(r'(.*)', code))  # 复用上面生成逻辑\n",
        "            # 只抽出我们生成的 if-block（从 \"if getattr...\" 到最后的 return）\n",
        "            block_only = block[ block.find('if getattr(self, \"epochs\", 0) <= 0:') : ]\n",
        "            lines[line_idx:ret_idx+1] = [block_only + \"\\n\"]\n",
        "            new_code = \"\".join(lines)\n",
        "\n",
        "# 写回\n",
        "tp.write_text(new_code, encoding=\"utf-8\")\n",
        "print(\"✅ 已把 epochs<=0 的分支替换为规范的“只推理一次并写结果”块（缩进严格）\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZmtMuY-LgvY",
        "outputId": "9da6601d-ecc7-4176-cfb5-23fee12b4895"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已把 epochs<=0 的分支替换为规范的“只推理一次并写结果”块（缩进严格）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "lines = code.splitlines(True)\n",
        "\n",
        "# 找到 def train(self...) 的起止范围（直到同级别的下一个 def/class 或 EOF）\n",
        "start_i = None\n",
        "for i, ln in enumerate(lines):\n",
        "    if ln.lstrip().startswith(\"def train(\") and ln.strip().endswith(\"):\"):\n",
        "        start_i = i\n",
        "        break\n",
        "\n",
        "assert start_i is not None, \"没在 src/train_pipeline.py 找到 def train(self, ...)。请把文件前200行贴给我。\"\n",
        "\n",
        "# 计算 train 定义行的缩进\n",
        "indent = lines[start_i][:len(lines[start_i]) - len(lines[start_i].lstrip())]\n",
        "\n",
        "# 找到 train 函数的结束位置：遇到同级的 'def ' 或 'class ' 就结束\n",
        "end_i = len(lines)\n",
        "for j in range(start_i + 1, len(lines)):\n",
        "    l = lines[j]\n",
        "    if l.startswith(indent) and l.lstrip().startswith((\"def \", \"class \")):\n",
        "        end_i = j\n",
        "        break\n",
        "\n",
        "# 用一个“仅推理一次并写结果”的规范实现替换整个函数\n",
        "new_block = f\"\"\"{indent}def train(self):\n",
        "{indent}    \\\"\\\"\\\"Inference-only run: build tracks, ensure model, run metrics once, write results.\\\"\\\"\\\"\n",
        "{indent}    try:\n",
        "{indent}        logger.info(\"train(): inference-only path\")\n",
        "{indent}    except Exception:\n",
        "{indent}        pass\n",
        "{indent}\n",
        "{indent}    # 1) 构建 tracks（带 metrics_config，确保写入结果目录）\n",
        "{indent}    self.tracks = [Track(track, self.config, metrics_config=self.metrics_config) for track in self.tracks]\n",
        "{indent}\n",
        "{indent}    # 2) 确保 model 存在（优先调用内部构建函数，其次兜底直接实例化 SORBET）\n",
        "{indent}    if getattr(self, \"model\", None) is None:\n",
        "{indent}        built = False\n",
        "{indent}        for _name in (\"build_model\", \"_build_model\", \"init_model\", \"create_model\"):\n",
        "{indent}            _fn = getattr(self, _name, None)\n",
        "{indent}            if callable(_fn):\n",
        "{indent}                try:\n",
        "{indent}                    _m = _fn()\n",
        "{indent}                    if _m is not None:\n",
        "{indent}                        self.model = _m\n",
        "{indent}                    built = True\n",
        "{indent}                    break\n",
        "{indent}                except Exception:\n",
        "{indent}                    pass\n",
        "{indent}        if getattr(self, \"model\", None) is None and not built:\n",
        "{indent}            SorbetModel = None\n",
        "{indent}            try:\n",
        "{indent}                from models.sorbet import SORBET as SorbetModel\n",
        "{indent}            except Exception:\n",
        "{indent}                try:\n",
        "{indent}                    from models.sorbet import Sorbet as SorbetModel\n",
        "{indent}                except Exception:\n",
        "{indent}                    SorbetModel = None\n",
        "{indent}            if SorbetModel is not None:\n",
        "{indent}                try:\n",
        "{indent}                    self.model = SorbetModel(config=self.config)\n",
        "{indent}                except Exception:\n",
        "{indent}                    try:\n",
        "{indent}                        self.model = SorbetModel()\n",
        "{indent}                    except Exception:\n",
        "{indent}                        self.model = None\n",
        "{indent}\n",
        "{indent}    # 3) 把模型挂到 inference_pipeline（如果可行）\n",
        "{indent}    try:\n",
        "{indent}        setattr(self.inference_pipeline, \"model\", self.model)\n",
        "{indent}    except Exception:\n",
        "{indent}        pass\n",
        "{indent}\n",
        "{indent}    # 4) 执行一次推理/评测并写入结果\n",
        "{indent}    results = self.run_metrics(self.tracks, debug=True, epoch=-1)\n",
        "{indent}    self.metrics_results = results\n",
        "{indent}    try:\n",
        "{indent}        logger.info(\"inference-only run finished; results written.\")\n",
        "{indent}    except Exception:\n",
        "{indent}        pass\n",
        "\"\"\"\n",
        "\n",
        "# 进行替换并写回\n",
        "lines[start_i:end_i] = [new_block]\n",
        "tp.write_text(\"\".join(lines), encoding=\"utf-8\")\n",
        "print(\"✅ 已将 train(self, ...) 替换为规范的“仅推理并写结果”版本。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogRT5DeDL7Rf",
        "outputId": "b3593c6b-f3bf-4b0c-d908-8d7046de4019"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已将 train(self, ...) 替换为规范的“仅推理并写结果”版本。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 在函数开头插入：writer 为空则直接返回（幂等，重复执行也不会出问题）\n",
        "guard_pat = r'(def\\s+_write_onto_embeddings_tensorboard\\([^\\)]*\\):\\s*\\n)'\n",
        "guard_ins = (\n",
        "    r'\\1'\n",
        "    r'        # Guard for inference-only runs: skip if no TensorBoard writer\\n'\n",
        "    r'        if getattr(self, \"tensorboard_writer\", None) is None:\\n'\n",
        "    r'            return\\n'\n",
        ")\n",
        "new_code, n = re.subn(guard_pat, guard_ins, code, count=1)\n",
        "if n == 0:\n",
        "    print(\"⚠️ 没找到 _write_onto_embeddings_tensorboard()，请把 src/train_pipeline.py 贴我前 300 行。\")\n",
        "else:\n",
        "    tp.write_text(new_code, encoding=\"utf-8\")\n",
        "    print(\"✅ 已为 _write_onto_embeddings_tensorboard() 加入安全保护\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtRPY_HoMXNN",
        "outputId": "3514b0a9-1af0-464f-defd-204c23f52e91"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已为 _write_onto_embeddings_tensorboard() 加入安全保护\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "from pathlib import Path, re\n",
        "\n",
        "tp = Path(\"src/train_pipeline.py\")\n",
        "code = tp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# 把所有： self._write_onto_embeddings_tensorboard( ... )\n",
        "# 改成：\n",
        "# if getattr(self, \"tensorboard_writer\", None) is not None:\n",
        "#     try:\n",
        "#         self._write_onto_embeddings_tensorboard( ... )\n",
        "#     except Exception:\n",
        "#         pass\n",
        "pattern = re.compile(r'(?m)^(?P<indent>\\s*)self\\._write_onto_embeddings_tensorboard\\((?P<args>.*)\\)\\s*$')\n",
        "def repl(m):\n",
        "    ind = m.group('indent')\n",
        "    args = m.group('args')\n",
        "    return (\n",
        "        f'{ind}if getattr(self, \"tensorboard_writer\", None) is not None:\\n'\n",
        "        f'{ind}    try:\\n'\n",
        "        f'{ind}        self._write_onto_embeddings_tensorboard({args})\\n'\n",
        "        f'{ind}    except Exception:\\n'\n",
        "        f'{ind}        pass'\n",
        "    )\n",
        "new_code, n = pattern.subn(repl, code)\n",
        "tp.write_text(new_code, encoding=\"utf-8\")\n",
        "print(f\"✅ 已为 _write_onto_embeddings_tensorboard 的 {n} 处调用加上判空保护\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3WVrHiM9Ot",
        "outputId": "76fbd8ed-b77f-4354-9f30-5f8b855c09c0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "✅ 已为 _write_onto_embeddings_tensorboard 的 1 处调用加上判空保护\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SORBET_ISWC23\n",
        "!python src/train.py\n",
        "# 顶部看到 cuDNN/cuBLAS 的告警属于环境噪声，可以忽略。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR-6OZ3BFFKo",
        "outputId": "45f6b95f-9833-44ee-94fb-1f9fe7cc725c"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SORBET_ISWC23\n",
            "2025-10-10 22:18:17.230569: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760134697.250677   15829 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760134697.257113   15829 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760134697.272498   15829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760134697.272528   15829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760134697.272532   15829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760134697.272537   15829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[INFO - 2025-10-10 22:18:21] - Using gpu: True\n",
            "INFO:onto:Using gpu: True\n",
            "[INFO - 2025-10-10 22:18:21] - CUDA version: 12.6\n",
            "INFO:onto:CUDA version: 12.6\n",
            "[INFO - 2025-10-10 22:18:21] - Started training with run_id: {self.run_id}\n",
            "INFO:onto:Started training with run_id: {self.run_id}\n",
            "[INFO - 2025-10-10 22:18:21] - train(): inference-only path\n",
            "INFO:onto:train(): inference-only path\n",
            "* Owlready2 * WARNING: DataProperty http://www.w3.org/2006/time#inXSDDateTime belongs to more than one entity types: [owl.DeprecatedProperty, owl.DatatypeProperty]; I'm trying to fix it...\n",
            "* Owlready2 * WARNING: DataProperty http://www.w3.org/2006/time#xsdDateTime belongs to more than one entity types: [owl.DeprecatedProperty, owl.DatatypeProperty]; I'm trying to fix it...\n",
            "* Owlready2 * WARNING: DataProperty https://sargon-n5geh.netlify.app/ontology/1.0/object_properties/has_floor belongs to more than one entity types: [owl.ObjectProperty, owl.DatatypeProperty]; I'm trying to fix it...\n",
            "[INFO - 2025-10-10 22:18:27] - Metrics for track: energy_pair\n",
            "INFO:onto:Metrics for track: energy_pair\n",
            "[INFO - 2025-10-10 22:18:37] - #### Metrics ####\n",
            "INFO:onto:#### Metrics ####\n",
            "[INFO - 2025-10-10 22:18:37] - \n",
            "             0     1    2     3\n",
            "Threshold  0.5  0.55  0.6  0.65\n",
            "Precision  NaN   NaN  NaN   NaN\n",
            "Recall     NaN   NaN  NaN   NaN\n",
            "F1         NaN   NaN  NaN   NaN\n",
            "INFO:onto:\n",
            "             0     1    2     3\n",
            "Threshold  0.5  0.55  0.6  0.65\n",
            "Precision  NaN   NaN  NaN   NaN\n",
            "Recall     NaN   NaN  NaN   NaN\n",
            "F1         NaN   NaN  NaN   NaN\n",
            "[INFO - 2025-10-10 22:18:37] - Hits@1    : 0.0\n",
            "INFO:onto:Hits@1    : 0.0\n",
            "[INFO - 2025-10-10 22:18:37] - Hits@5    : 0.0\n",
            "INFO:onto:Hits@5    : 0.0\n",
            "[INFO - 2025-10-10 22:18:37] - Hits@10    : 0.0\n",
            "INFO:onto:Hits@10    : 0.0\n",
            "[INFO - 2025-10-10 22:18:38] - Eval time: 10.897050857543945\n",
            "INFO:onto:Eval time: 10.897050857543945\n",
            "[INFO - 2025-10-10 22:18:38] - inference-only run finished; results written.\n",
            "INFO:onto:inference-only run finished; results written.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "dst = \"/content/drive/MyDrive/sorbet_data/result_alignments\"\n",
        "hits = sorted(glob.glob(f\"{dst}/**/*\", recursive=True))\n",
        "print(\"\\n—— 写入目录 ——\")\n",
        "print(dst)\n",
        "print(\"找到结果文件数：\", len(hits))\n",
        "for h in hits:\n",
        "    if h.endswith((\".tsv\",\".rdf\")):\n",
        "        print(\" -\", os.path.relpath(h, dst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5jVIz1BNmMZ",
        "outputId": "92e58479-e221-4e74-f9dd-9eef3288081d"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "—— 写入目录 ——\n",
            "/content/drive/MyDrive/sorbet_data/result_alignments\n",
            "找到结果文件数： 15\n",
            " - energy_pair/Sargon2saref4bldg_-1.rdf\n",
            " - energy_pair/Sargon2saref4bldg_-1.tsv\n",
            " - energy_pair/Sargon2saref4bldg_-1_20251010-145758.rdf\n",
            " - energy_pair/Sargon2saref4bldg_-1_20251010-145758.tsv\n",
            " - energy_pair/Sargon2saref4bldg_-1_20251010-150505.rdf\n",
            " - energy_pair/Sargon2saref4bldg_-1_20251010-150505.tsv\n",
            " - energy_pair/Sargon2saref4bldg_-1_20251010-150843.rdf\n",
            " - energy_pair/Sargon2saref4bldg_-1_20251010-150843.tsv\n",
            " - energy_pair/saref4bldg2Sargon_-1.rdf\n",
            " - energy_pair/saref4bldg2Sargon_-1.tsv\n",
            " - run_20251010_145629/energy_pair/Sargon2saref4bldg_-1.rdf\n",
            " - run_20251010_145629/energy_pair/Sargon2saref4bldg_-1.tsv\n"
          ]
        }
      ]
    }
  ]
}